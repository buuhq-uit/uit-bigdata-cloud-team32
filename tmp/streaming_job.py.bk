import requests
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    from_json,
    to_timestamp,
    window,
    avg,
    stddev,
    max as ps_max,
    lit,
)
from pyspark.sql.types import StructType, StructField, StringType, MapType, DoubleType

# =========================
# Config (match docker-compose)
# =========================
INFLUX_URL = "http://influxdb:8086"
INFLUX_ORG = "iot-org"
INFLUX_BUCKET = "iot-bucket"
INFLUX_TOKEN = "iot-token-123"

PG_URL = "jdbc:postgresql://postgres:5432/iotdb"
PG_USER = "iot"
PG_PASS = "iotpass"

MINIO_ENDPOINT = "http://minio:9000"
MINIO_ACCESS = "minioadmin"
MINIO_SECRET = "minioadmin123"
MINIO_BUCKET = "iot-datalake"

KAFKA_BOOTSTRAP = "kafka:9092"
RAW_TOPIC = "iot.sensor.raw"

# =========================
# Schema
# =========================
schema = StructType(
    [
        StructField("ts", StringType(), True),
        StructField("device_id", StringType(), True),
        StructField("location", StringType(), True),
        StructField("model", StringType(), True),
        StructField("metrics", MapType(StringType(), DoubleType()), True),
    ]
)

# =========================
# Spark Session
# =========================
spark = SparkSession.builder.appName("iot-anomaly-streaming").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

# =========================
# MinIO (S3A) Hadoop config
# =========================
hconf = spark._jsc.hadoopConfiguration()
hconf.set("fs.s3a.endpoint", MINIO_ENDPOINT)
hconf.set("fs.s3a.access.key", MINIO_ACCESS)
hconf.set("fs.s3a.secret.key", MINIO_SECRET)
hconf.set("fs.s3a.path.style.access", "true")
hconf.set("fs.s3a.connection.ssl.enabled", "false")
# optional stability
hconf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# =========================
# Kafka Source
# =========================
raw = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP)
    .option("subscribe", RAW_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

parsed = (
    raw.selectExpr("CAST(value AS STRING) as value")
    .withColumn("json", from_json(col("value"), schema))
    .select("json.*")
    .withColumn("event_ts", to_timestamp(col("ts")))
    .withColumn("temperature", col("metrics").getItem("temperature"))
    .withColumn("humidity", col("metrics").getItem("humidity"))
    .drop("metrics", "ts")
)

# ==========================================================
# 1) RAW branch: archive to MinIO + write raw metrics to Influx
# ==========================================================
def archive_raw_to_minio(batch_df, batch_id):
    (
        batch_df.select(
            "event_ts", "device_id", "location", "model", "temperature", "humidity"
        )
        .write.mode("append")
        .json(f"s3a://{MINIO_BUCKET}/raw/")
    )


def write_raw_to_influx(batch_df, batch_id):
    rows = batch_df.select("event_ts", "device_id", "temperature", "humidity").collect()
    if not rows:
        return

    lines = []
    for r in rows:
        ts_ns = int(r["event_ts"].timestamp() * 1_000_000_000)
        device_id = r["device_id"]
        t = float(r["temperature"]) if r["temperature"] is not None else 0.0
        h = float(r["humidity"]) if r["humidity"] is not None else 0.0
        lines.append(
            f"sensor_metrics,device_id={device_id} temperature={t},humidity={h} {ts_ns}"
        )

    data = "\n".join(lines)
    url = f"{INFLUX_URL}/api/v2/write?org={INFLUX_ORG}&bucket={INFLUX_BUCKET}&precision=ns"
    headers = {
        "Authorization": f"Token {INFLUX_TOKEN}",
        "Content-Type": "text/plain; charset=utf-8",
    }
    resp = requests.post(url, headers=headers, data=data, timeout=10)
    resp.raise_for_status()


def foreach_raw(batch_df, batch_id):
    # archive data lake + time-series
    archive_raw_to_minio(batch_df, batch_id)
    write_raw_to_influx(batch_df, batch_id)


raw_query = (
    parsed.writeStream.foreachBatch(foreach_raw)
    .outputMode("update")
    .option("checkpointLocation", "/opt/spark-app/checkpoints/raw")
    .start()
)

# ==========================================================
# 2) ANOMALY branch: window aggregation (NO stream-stream join)
# ==========================================================
# Window: 1 minute, sliding 10 seconds
# Score: (max_temp - avg_temp) / std_temp
agg = (
    parsed.withWatermark("event_ts", "2 minutes")
    .groupBy(window(col("event_ts"), "1 minute", "10 seconds"), col("device_id"))
    .agg(
        avg("temperature").alias("avg_temp"),
        stddev("temperature").alias("std_temp"),
        ps_max("temperature").alias("max_temp"),
    )
)

anomalies = (
    agg.filter(col("std_temp").isNotNull() & (col("std_temp") > 0))
    .withColumn("score", (col("max_temp") - col("avg_temp")) / col("std_temp"))
    .filter(col("score") >= 4)
    .select(
        col("window.end").alias("ts"),
        col("device_id"),
        lit("temperature").alias("metric"),
        col("max_temp").alias("value"),
        col("score").alias("score"),
        lit("zscore_window_1m").alias("method"),
        lit("high").alias("severity"),
    )
)

# ---------- sinks ----------
def write_anom_to_postgres(batch_df, batch_id):
    (
        batch_df.write.format("jdbc")
        .mode("append")
        .option("url", PG_URL)
        .option("dbtable", "anomalies")
        .option("user", PG_USER)
        .option("password", PG_PASS)
        .option("driver", "org.postgresql.Driver")
        .save()
    )


def write_anom_to_influx(batch_df, batch_id):
    rows = batch_df.collect()
    if not rows:
        return

    lines = []
    for r in rows:
        ts_ns = int(r["ts"].timestamp() * 1_000_000_000)
        device_id = r["device_id"]
        metric = r["metric"]
        v = float(r["value"])
        s = float(r["score"])
        lines.append(
            f"sensor_anomalies,device_id={device_id},metric={metric} value={v},score={s} {ts_ns}"
        )

    data = "\n".join(lines)
    url = f"{INFLUX_URL}/api/v2/write?org={INFLUX_ORG}&bucket={INFLUX_BUCKET}&precision=ns"
    headers = {
        "Authorization": f"Token {INFLUX_TOKEN}",
        "Content-Type": "text/plain; charset=utf-8",
    }
    resp = requests.post(url, headers=headers, data=data, timeout=10)
    resp.raise_for_status()


def foreach_anom(batch_df, batch_id):
    write_anom_to_postgres(batch_df, batch_id)
    write_anom_to_influx(batch_df, batch_id)


anom_query = (
    anomalies.writeStream.foreachBatch(foreach_anom)
    .outputMode("append")  # IMPORTANT: append for aggregation stream
    .option("checkpointLocation", "/opt/spark-app/checkpoints/anom")
    .start()
)

spark.streams.awaitAnyTermination()
